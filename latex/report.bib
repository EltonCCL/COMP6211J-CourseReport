@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}


@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the {OpenAI} {API}, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune {GPT}-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models {InstructGPT}. In human evaluations on our prompt distribution, outputs from the 1.3B parameter {InstructGPT} model are preferred to outputs from the 175B {GPT}-3, despite having 100x fewer parameters. Moreover, {InstructGPT} models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public {NLP} datasets. Even though {InstructGPT} still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	number = {{arXiv}:2203.02155},
	publisher = {{arXiv}},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	urldate = {2024-10-31},
	date = {2022-03-04},
	eprinttype = {arxiv},
	eprint = {2203.02155},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/XD37BXJN/Training language models to follow instructions with human feedback - 2022.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/7K7XNV7H/2203.html:text/html},
}

@inproceedings{NIPS2017_d5e2c0ad,
	author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Deep Reinforcement Learning from Human Preferences},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},
	volume = {30},
	year = {2017}
}

@inproceedings{macglashan_interactive_2017,
	title = {Interactive Learning from Policy-Dependent Human Feedback},
	url = {https://proceedings.mlr.press/v70/macglashan17a.html},
	abstract = {This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner’s current policy. We present empirical results that show this assumption to be false—whether human trainers give a positive or negative feedback for a decision is influenced by the learner’s current policy. Based on this insight, we introduce Convergent Actor-Critic by Humans ({COACH}), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that {COACH} can successfully learn multiple behaviors on a physical robot.},
	eventtitle = {International Conference on Machine Learning},
	pages = {2285--2294},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {{MacGlashan}, James and Ho, Mark K. and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L. and Taylor, Matthew E. and Littman, Michael L.},
	urldate = {2024-11-10},
	date = {2017-07-17},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	keywords = {/unread},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/3LX8BY6S/Interactive Learning from Policy-Dependent Human Feedback - 2017.pdf:application/pdf},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the {OpenAI} {API}, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune {GPT}-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models {InstructGPT}. In human evaluations on our prompt distribution, outputs from the 1.3B parameter {InstructGPT} model are preferred to outputs from the 175B {GPT}-3, despite having 100x fewer parameters. Moreover, {InstructGPT} models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public {NLP} datasets. Even though {InstructGPT} still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	number = {{arXiv}:2203.02155},
	publisher = {{arXiv}},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	urldate = {2024-10-31},
	date = {2022-03-04},
	eprinttype = {arxiv},
	eprint = {2203.02155},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/XD37BXJN/Training language models to follow instructions with human feedback - 2022.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/7K7XNV7H/2203.html:text/html},
}


@misc{stiennon_learning_2022,
	title = {Learning to summarize from human feedback},
	url = {http://arxiv.org/abs/2009.01325},
	abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using {ROUGE}, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the {TL};{DR} dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to {CNN}/{DM} news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing {ROUGE} according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
	number = {{arXiv}:2009.01325},
	publisher = {{arXiv}},
	author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
	urldate = {2024-11-10},
	date = {2022-02-15},
	eprinttype = {arxiv},
	eprint = {2009.01325},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/IS6QZQHS/Learning to summarize from human feedback - 2022.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/3P24T3KS/2009.html:text/html},
}


@misc{zhang_instruction_2024,
	title = {Instruction Tuning for Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2308.10792},
	shorttitle = {Instruction Tuning for Large Language Models},
	abstract = {This paper surveys research works in the quickly advancing field of instruction tuning ({IT}), a crucial technique to enhance the capabilities and controllability of large language models ({LLMs}). Instruction tuning refers to the process of further training {LLMs} on a dataset consisting of {\textbackslash}textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of {LLMs} and the users' objective of having {LLMs} adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of {IT}, the construction of {IT} datasets, the training of {IT} models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of {IT} (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of {IT} along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey},
	number = {{arXiv}:2308.10792},
	publisher = {{arXiv}},
	author = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
	urldate = {2024-11-10},
	date = {2024-10-16},
	eprinttype = {arxiv},
	eprint = {2308.10792},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/K4HW2CDZ/Instruction Tuning for Large Language Models A Survey - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/W9PBNDZP/2308.html:text/html},
}



@misc{rafailov_direct_2024,
	title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
	url = {http://arxiv.org/abs/2305.18290},
	doi = {10.48550/arXiv.2305.18290},
	shorttitle = {Direct Preference Optimization},
	abstract = {While large-scale unsupervised language models ({LMs}) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised {LM} to align with these preferences, often with reinforcement learning from human feedback ({RLHF}). However, {RLHF} is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised {LM} using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in {RLHF} that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard {RLHF} problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization ({DPO}), is stable, performant, and computationally lightweight, eliminating the need for sampling from the {LM} during fine-tuning or performing significant hyperparameter tuning. Our experiments show that {DPO} can fine-tune {LMs} to align with human preferences as well as or better than existing methods. Notably, fine-tuning with {DPO} exceeds {PPO}-based {RLHF} in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	number = {{arXiv}:2305.18290},
	publisher = {{arXiv}},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	urldate = {2024-11-15},
	date = {2024-07-29},
	eprinttype = {arxiv},
	eprint = {2305.18290},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/4DTHFX64/Direct Preference Optimization Your Language Model is Secretly a Reward Model - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/D4LR8KPP/2305.html:text/html},
}


@misc{wang_comprehensive_2024,
	title = {A Comprehensive Survey of {LLM} Alignment Techniques: {RLHF}, {RLAIF}, {PPO}, {DPO} and More},
	url = {http://arxiv.org/abs/2407.16216},
	shorttitle = {A Comprehensive Survey of {LLM} Alignment Techniques},
	abstract = {With advancements in self-supervised learning, the availability of trillions tokens in a pre-training corpus, instruction fine-tuning, and the development of large Transformers with billions of parameters, large language models ({LLMs}) are now capable of generating factual and coherent responses to human queries. However, the mixed quality of training data can lead to the generation of undesired responses, presenting a significant challenge. Over the past two years, various methods have been proposed from different perspectives to enhance {LLMs}, particularly in aligning them with human expectation. Despite these efforts, there has not been a comprehensive survey paper that categorizes and details these approaches. In this work, we aim to address this gap by categorizing these papers into distinct topics and providing detailed explanations of each alignment method, thereby helping readers gain a thorough understanding of the current state of the field.},
	number = {{arXiv}:2407.16216},
	publisher = {{arXiv}},
	author = {Wang, Zhichao and Bi, Bin and Pentyala, Shiva Kumar and Ramnath, Kiran and Chaudhuri, Sougata and Mehrotra, Shubham and Zixu and Zhu and Mao, Xiang-Bo and Asur, Sitaram and Na and Cheng},
	urldate = {2024-10-29},
	date = {2024-07-23},
	eprinttype = {arxiv},
	eprint = {2407.16216},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/LKYQHRSL/A Comprehensive Survey of LLM Alignment Techniques RLHF, RLAIF, PPO, DPO and More - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/29ID337E/2407.html:text/html},
}


@misc{xu_is_2024,
	title = {Is {DPO} Superior to {PPO} for {LLM} Alignment? A Comprehensive Study},
	url = {http://arxiv.org/abs/2404.10719},
	shorttitle = {Is {DPO} Superior to {PPO} for {LLM} Alignment?},
	abstract = {Reinforcement Learning from Human Feedback ({RLHF}) is currently the most widely used method to align large language models ({LLMs}) with human preferences. Existing {RLHF} methods can be roughly categorized as either reward-based or reward-free. Novel applications such as {ChatGPT} and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization ({PPO}). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization ({DPO}). Is {DPO} truly superior to {PPO}? Why does {PPO} perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of {DPO} and show that {DPO} may have fundamental limitations. Moreover, we also comprehensively examine {PPO} and reveal the key factors for the best performances of {PPO} in fine-tuning {LLMs}. Finally, we benchmark {DPO} and {PPO} across a collection of {RLHF} testbeds, ranging from dialogue to code generation. Experiment results demonstrate that {PPO} is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions. Our code is publicly available at https://github.com/openpsi-project/{ReaLHF}.},
	number = {{arXiv}:2404.10719},
	publisher = {{arXiv}},
	author = {Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi},
	urldate = {2024-11-16},
	date = {2024-10-10},
	eprinttype = {arxiv},
	eprint = {2404.10719},
	keywords = {/unread, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/2ESW9SSL/Is DPO Superior to PPO for LLM Alignment A Comprehensive Study - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/AS8MEEZW/2404.html:text/html},
}


@misc{ahmadian_back_2024,
	title = {Back to Basics: Revisiting {REINFORCE} Style Optimization for Learning from Human Feedback in {LLMs}},
	url = {http://arxiv.org/abs/2402.14740},
	shorttitle = {Back to Basics},
	abstract = {{AI} alignment in the shape of Reinforcement Learning from Human Feedback ({RLHF}) is increasingly treated as a crucial ingredient for high performance large language models. {\textbackslash}textsc\{Proximal Policy Optimization\} ({PPO}) has been positioned by recent literature as the canonical method for the {RL} part of {RLHF}. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of {PPO} are less of a practical concern in {RLHF} and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the {\textbackslash}textit\{formulation\} of alignment from human preferences in the context of {RL}. Keeping simplicity as a guiding principle, we show that many components of {PPO} are unnecessary in an {RLHF} context and that far simpler {REINFORCE}-style optimization variants outperform both {PPO} and newly proposed "{RL}-free" methods such as {DPO} and {RAFT}. Our work suggests that careful adaptation to {LLMs} alignment characteristics enables benefiting from online {RL} optimization at low cost.},
	number = {{arXiv}:2402.14740},
	publisher = {{arXiv}},
	author = {Ahmadian, Arash and Cremer, Chris and Gallé, Matthias and Fadaee, Marzieh and Kreutzer, Julia and Üstün, Ahmet and Hooker, Sara},
	urldate = {2024-11-16},
	date = {2024-02-22},
	eprinttype = {arxiv},
	eprint = {2402.14740},
	note = {version: 1},
	keywords = {/unread, Computer Science - Machine Learning},
}


@misc{lin_flame_2024,
	title = {{FLAME}: Factuality-Aware Alignment for Large Language Models},
	url = {http://arxiv.org/abs/2405.01525},
	shorttitle = {{FLAME}},
	abstract = {Alignment is a standard procedure to fine-tune pre-trained large language models ({LLMs}) to follow natural language instructions and serve as helpful {AI} assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of {LLMs}, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the {LLM} alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:{\textbackslash} supervised fine-tuning ({SFT}) and reinforcement learning ({RL}). In particular, we find that training the {LLM} on new knowledge or unfamiliar texts can encourage hallucination. This makes {SFT} less factual as it trains on human labeled data that may be novel to the {LLM}. Furthermore, reward functions used in standard {RL} can also encourage hallucination, because it guides the {LLM} to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware {SFT} and factuality-aware {RL} through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides {LLMs} to output more factual responses while maintaining instruction-following capability.},
	number = {{arXiv}:2405.01525},
	publisher = {{arXiv}},
	author = {Lin, Sheng-Chieh and Gao, Luyu and Oguz, Barlas and Xiong, Wenhan and Lin, Jimmy and Yih, Wen-tau and Chen, Xilun},
	urldate = {2024-10-31},
	date = {2024-05-02},
	eprinttype = {arxiv},
	eprint = {2405.01525},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/7KPPH3BG/FLAME Factuality-Aware Alignment for Large Language Models - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/SGCHYU5N/2405.html:text/html},
}

@misc{min_factscore_2023,
	title = {{FActScore}: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation},
	url = {http://arxiv.org/abs/2305.14251},
	shorttitle = {{FActScore}},
	abstract = {Evaluating the factuality of long-form text generated by large language models ({LMs}) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce {FACTSCORE}, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain {FACTSCOREs} of people biographies generated by several state-of-the-art commercial {LMs} -- {InstructGPT}, {ChatGPT}, and the retrieval-augmented {PerplexityAI} -- and report new analysis demonstrating the need for such a fine-grained score (e.g., {ChatGPT} only achieves 58\%). Since human evaluation is costly, we also introduce an automated model that estimates {FACTSCORE} using retrieval and a strong language model, with less than a 2\% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent {LMs} that would have cost \$26K if evaluated by humans, with various findings: {GPT}-4 and {ChatGPT} are more factual than public models, and Vicuna and Alpaca are some of the best public models. {FACTSCORE} is available for public use via `pip install factscore`.},
	number = {{arXiv}:2305.14251},
	publisher = {{arXiv}},
	author = {Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
	urldate = {2024-11-16},
	date = {2023-10-11},
	eprinttype = {arxiv},
	eprint = {2305.14251},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/2V4R2WI3/FActScore Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation - 2023.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/NQTVV2NE/2305.html:text/html},
}


@misc{wangFactualityLargeLanguage2024,
	title = {Factuality of Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2402.02420},
	shorttitle = {Factuality of Large Language Models},
	abstract = {Large language models ({LLMs}), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, {LLM} responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of {LLMs} has attracted a lot of attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of {LLMs}, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.},
	number = {{arXiv}:2402.02420},
	publisher = {{arXiv}},
	author = {Wang, Yuxia and Wang, Minghan and Manzoor, Muhammad Arslan and Liu, Fei and Georgiev, Georgi and Das, Rocktim Jyoti and Nakov, Preslav},
	urldate = {2024-11-16},
	date = {2024-10-31},
	eprinttype = {arxiv},
	eprint = {2402.02420},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/2MWJYJRJ/Factuality of Large Language Models A Survey - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/3GEJQQ6J/2402.html:text/html},
}


@misc{brownLanguageModelsAre2020,
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.},
	number = {{arXiv}:2005.14165},
	publisher = {{arXiv}},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2024-11-16},
	date = {2020-07-22},
	eprinttype = {arxiv},
	eprint = {2005.14165},
	keywords = {/unread, Computer Science - Computation and Language},
}

@article{weng2024hallucination,
  title   = "Extrinsic Hallucinations in LLMs.",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2024",
  month   = "Jul",
  url     = "https://lilianweng.github.io/posts/2024-07-07-hallucination/"
}


@misc{duImprovingFactualityReasoning2023,
	title = {Improving Factuality and Reasoning in Language Models through Multiagent Debate},
	url = {http://arxiv.org/abs/2305.14325},
	abstract = {Large language models ({LLMs}) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such "society of minds" approach has the potential to significantly advance the capabilities of {LLMs} and pave the way for further breakthroughs in language generation and understanding.},
	number = {{arXiv}:2305.14325},
	publisher = {{arXiv}},
	author = {Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B. and Mordatch, Igor},
	urldate = {2024-11-16},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.14325},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/LJQ7G4NC/Improving Factuality and Reasoning in Language Models through Multiagent Debate - 2023.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/LHHBEFMD/2305.html:text/html},
}


@misc{huangSurveyHallucinationLarge2023,
	title = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
	url = {http://arxiv.org/abs/2311.05232},
	doi = {10.48550/arXiv.2311.05232},
	shorttitle = {A Survey on Hallucination in Large Language Models},
	abstract = {The emergence of large language models ({LLMs}) has marked a significant breakthrough in natural language processing ({NLP}), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, {LLMs} exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of {LLMs} in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of {LLM} hallucinations. We begin with an innovative taxonomy of {LLM} hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in {LLMs}.},
	number = {{arXiv}:2311.05232},
	publisher = {{arXiv}},
	author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
	urldate = {2024-10-25},
	date = {2023-11-09},
	eprinttype = {arxiv},
	eprint = {2311.05232},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/5QW2AIAD/A Survey on Hallucination in Large Language Models Principles, Taxonomy, Challenges, and Open Quest - 2023.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/9WVDMCD3/2311.html:text/html},
}


@misc{rawteTroublingEmergenceHallucination2023,
	title = {The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quantification, and Prescriptive Remediations},
	url = {http://arxiv.org/abs/2310.04988},
	abstract = {The recent advancements in Large Language Models ({LLMs}) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage ({FM}) and (ii) silver lining ({SL}). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate {HallucInation} {eLiciTation} ({HILT}), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary {LLMs} along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank {LLMs} based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index ({HVI}). We firmly believe that {HVI} holds significant value as a tool for the wider {NLP} community, with the potential to serve as a rubric in {AI}-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.},
	number = {{arXiv}:2310.04988},
	publisher = {{arXiv}},
	author = {Rawte, Vipula and Chakraborty, Swagata and Pathak, Agnibh and Sarkar, Anubhav and Tonmoy, S. M. Towhidul Islam and Chadha, Aman and Sheth, Amit P. and Das, Amitava},
	urldate = {2024-11-16},
	date = {2023-10-23},
	eprinttype = {arxiv},
	eprint = {2310.04988},
	keywords = {/unread, Computer Science - Artificial Intelligence},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/7QD2JT6N/The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quanti - 2023.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/6GPSS5FY/2310.html:text/html},
}


@misc{openaiGPT4TechnicalReport2024,
	title = {{GPT}-4 Technical Report},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
	number = {{arXiv}:2303.08774},
	publisher = {{arXiv}},
	author = {{OpenAI} and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and {McGrew}, Bob and {McKinney}, Scott Mayer and {McLeavey}, Christine and {McMillan}, Paul and {McNeil}, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	urldate = {2024-11-16},
	date = {2024-03-04},
	eprinttype = {arxiv},
	eprint = {2303.08774},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/3STEDW3U/GPT-4 Technical Report - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/RQJWZH2F/2303.html:text/html},
}


@misc{dubeyLlama3Herd2024,
	title = {The Llama 3 Herd of Models},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence ({AI}) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as {GPT}-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	number = {{arXiv}:2407.21783},
	publisher = {{arXiv}},
	author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and {McConnell}, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and {AlBadawy}, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and Linde, Jelmer van der and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Rantala-Yeary, Lauren and Maaten, Laurens van der and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and Oliveira, Luke de and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzmán, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and {McPhie}, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Albiero, Vítor and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and {DeVito}, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
	urldate = {2024-11-16},
	date = {2024-08-15},
	eprinttype = {arxiv},
	eprint = {2407.21783},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/JMIST5G2/The Llama 3 Herd of Models - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/DQPJ3E7R/2407.html:text/html},
}


@misc{kaddourChallengesApplicationsLarge2023,
	title = {Challenges and Applications of Large Language Models},
	url = {http://arxiv.org/abs/2307.10169},
	abstract = {Large Language Models ({LLMs}) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that {ML} researchers can comprehend the field's current state more quickly and become productive.},
	number = {{arXiv}:2307.10169},
	publisher = {{arXiv}},
	author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and {McHardy}, Robert},
	urldate = {2024-11-16},
	date = {2023-07-19},
	eprinttype = {arxiv},
	eprint = {2307.10169},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}



@article{SurveyHallucinationNaturalLanguageGeneration2023,
	title = {Survey of Hallucination in Natural Language Generation},
	volume = {55},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3571730},
	doi = {10.1145/3571730},
	abstract = {Natural Language Generation ({NLG}) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent {NLG}, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in {NLG}. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in {NLG}.},
	pages = {248:1--248:38},
	number = {12},
	journaltitle = {{ACM} Comput. Surv.},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
	urldate = {2024-11-24},
	date = {2023-03-03},
	keywords = {/unread},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/LHJVFTZH/Survey of Hallucination in Natural Language Generation - 2023.pdf:application/pdf},
}



@misc{LanguageModelsAreFewShotLearners2020,
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.},
	number = {{arXiv}:2005.14165},
	publisher = {{arXiv}},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2024-11-17},
	date = {2020-07-22},
	eprinttype = {arxiv},
	eprint = {2005.14165},
	keywords = {/unread, Computer Science - Computation and Language},
}


@misc{LLaMAOpenEfficientFoundationLanguage2023,
	title = {{LLaMA}: Open and Efficient Foundation Language Models},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	shorttitle = {{LLaMA}},
	abstract = {We introduce {LLaMA}, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, {LLaMA}-13B outperforms {GPT}-3 (175B) on most benchmarks, and {LLaMA}-65B is competitive with the best models, Chinchilla-70B and {PaLM}-540B. We release all our models to the research community.},
	number = {{arXiv}:2302.13971},
	publisher = {{arXiv}},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	urldate = {2024-11-17},
	date = {2023-02-27},
	eprinttype = {arxiv},
	eprint = {2302.13971},
	keywords = {/unread, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/CJTXKKML/LLaMA Open and Efficient Foundation Language Models - 2023.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/CVCSNNUW/2302.html:text/html},
}


@misc{rawteTroublingEmergenceHallucination2023,
	title = {The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quantification, and Prescriptive Remediations},
	url = {http://arxiv.org/abs/2310.04988},
	abstract = {The recent advancements in Large Language Models ({LLMs}) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage ({FM}) and (ii) silver lining ({SL}). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate {HallucInation} {eLiciTation} ({HILT}), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary {LLMs} along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank {LLMs} based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index ({HVI}). We firmly believe that {HVI} holds significant value as a tool for the wider {NLP} community, with the potential to serve as a rubric in {AI}-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.},
	number = {{arXiv}:2310.04988},
	publisher = {{arXiv}},
	author = {Rawte, Vipula and Chakraborty, Swagata and Pathak, Agnibh and Sarkar, Anubhav and Tonmoy, S. M. Towhidul Islam and Chadha, Aman and Sheth, Amit P. and Das, Amitava},
	urldate = {2024-11-16},
	date = {2023-10-23},
	eprinttype = {arxiv},
	eprint = {2310.04988},
	keywords = {/unread, Computer Science - Artificial Intelligence},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/7QD2JT6N/The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quanti - 2023.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/6GPSS5FY/2310.html:text/html},
}


@misc{SurveyEvaluationLargeLanguageModels2023,
	title = {A Survey on Evaluation of Large Language Models},
	url = {http://arxiv.org/abs/2307.03109},
	abstract = {Large language models ({LLMs}) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As {LLMs} continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine {LLMs} from various perspectives. This paper presents a comprehensive review of these evaluation methods for {LLMs}, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of {LLMs}. Then, we summarize the success and failure cases of {LLMs} in different tasks. Finally, we shed light on several future challenges that lie ahead in {LLMs} evaluation. Our aim is to offer invaluable insights to researchers in the realm of {LLMs} evaluation, thereby aiding the development of more proficient {LLMs}. Our key point is that evaluation should be treated as an essential discipline to better assist the development of {LLMs}. We consistently maintain the related open-source materials at: https://github.com/{MLGroupJLU}/{LLM}-eval-survey.},
	number = {{arXiv}:2307.03109},
	publisher = {{arXiv}},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	urldate = {2024-11-17},
	date = {2023-12-29},
	eprinttype = {arxiv},
	eprint = {2307.03109},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/P8YUXIIV/A Survey on Evaluation of Large Language Models - 2023.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/2IEW6ZJ3/2307.html:text/html},
}


@misc{FaithfulnessFactualityAbstractiveSummarization2020,
	title = {On Faithfulness and Factuality in Abstractive Summarization},
	url = {http://arxiv.org/abs/2005.00661},
	abstract = {It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., {ROUGE}, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.},
	number = {{arXiv}:2005.00661},
	publisher = {{arXiv}},
	author = {Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and {McDonald}, Ryan},
	urldate = {2024-11-17},
	date = {2020-05-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.00661 [cs]},
	keywords = {/unread, Computer Science - Computation and Language},
	file = {PDF:/Users/eltonli/Zotero/storage/3FRZ95MN/On Faithfulness and Factuality in Abstractive Summarization - 2020.pdf:application/pdf},
}


@misc{FinetunedLanguageModelsAreZeroShot2022,
	title = {Finetuned Language Models Are Zero-Shot Learners},
	url = {http://arxiv.org/abs/2109.01652},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 {NLP} tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call {FLAN}, on unseen task types. {FLAN} substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B {GPT}-3 on 20 of 25 tasks that we evaluate. {FLAN} even outperforms few-shot {GPT}-3 by a large margin on {ANLI}, {RTE}, {BoolQ}, {AI}2-{ARC}, {OpenbookQA}, and {StoryCloze}. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
	number = {{arXiv}:2109.01652},
	publisher = {{arXiv}},
	author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
	urldate = {2024-11-17},
	date = {2022-02-08},
	eprinttype = {arxiv},
	eprint = {2109.01652},
	keywords = {/unread, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/JYHPXLKL/Finetuned Language Models Are Zero-Shot Learners - 2022.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/JTVIWWQL/2109.html:text/html},
}


@misc{AlignmentHonesty2024,
	title = {Alignment for Honesty},
	url = {http://arxiv.org/abs/2312.07000},
	doi = {10.48550/arXiv.2312.07000},
	abstract = {Recent research has made significant strides in aligning large language models ({LLMs}) with helpfulness and harmlessness. In this paper, we argue for the importance of alignment for {\textbackslash}emph\{honesty\}, ensuring that {LLMs} proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning an {LLM}'s knowledge boundaries, which demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. We address these challenges by first establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an {LLM}'s honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source all relevant resources to facilitate future research at {\textbackslash}url\{https://github.com/{GAIR}-{NLP}/alignment-for-honesty\}.},
	number = {{arXiv}:2312.07000},
	publisher = {{arXiv}},
	author = {Yang, Yuqing and Chern, Ethan and Qiu, Xipeng and Neubig, Graham and Liu, Pengfei},
	urldate = {2024-11-17},
	date = {2024-10-28},
	eprinttype = {arxiv},
	eprint = {2312.07000},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/B86EB5AA/Alignment for Honesty - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/XXZ72ALA/2312.html:text/html},
}


@misc{RTuningInstructingLargeLanguageModels2024,
	title = {R-Tuning: Instructing Large Language Models to Say `I Don't Know'},
	url = {http://arxiv.org/abs/2311.09677},
	shorttitle = {R-Tuning},
	abstract = {Large language models ({LLMs}) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the disparity in knowledge encompassed by pre-trained parameters compared to that of instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune {LLMs} to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate R-Tuning effectively improves a model's ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty results in better calibration and an improved ability to estimate the uncertainty than uncertainty-based testing. Our code is available at https://github.com/shizhediao/R-Tuning.},
	number = {{arXiv}:2311.09677},
	publisher = {{arXiv}},
	author = {Zhang, Hanning and Diao, Shizhe and Lin, Yong and Fung, Yi R. and Lian, Qing and Wang, Xingyao and Chen, Yangyi and Ji, Heng and Zhang, Tong},
	urldate = {2024-11-17},
	date = {2024-06-07},
	eprinttype = {arxiv},
	eprint = {2311.09677},
	keywords = {/unread, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/VW8HM7AY/R-Tuning Instructing Large Language Models to Say `I Don't Know' - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/7JAY747Q/2311.html:text/html},
}


@misc{CanAIAssistantsKnowWhat2024,
	title = {Can {AI} Assistants Know What They Don't Know?},
	url = {http://arxiv.org/abs/2401.13275},
	abstract = {Recently, {AI} assistants based on large language models ({LLMs}) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools. Although {LLMs} possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the {AI} assistant may cause significant risks in practical applications. We believe that an {AI} assistant's refusal to answer questions it does not know is a crucial method for reducing hallucinations and making the assistant truthful. Therefore, in this paper, we ask the question "Can {AI} assistants know what they don't know and express them through natural language?" To answer this question, we construct a model-specific "I don't know" (Idk) dataset for an assistant, which contains its known and unknown questions, based on existing open-domain question answering datasets. Then we align the assistant with its corresponding Idk dataset and observe whether it can refuse to answer its unknown questions after alignment. Experimental results show that after alignment with Idk datasets, the assistant can refuse to answer most its unknown questions. For questions they attempt to answer, the accuracy is significantly higher than before the alignment.},
	number = {{arXiv}:2401.13275},
	publisher = {{arXiv}},
	author = {Cheng, Qinyuan and Sun, Tianxiang and Liu, Xiangyang and Zhang, Wenwei and Yin, Zhangyue and Li, Shimin and Li, Linyang and He, Zhengfu and Chen, Kai and Qiu, Xipeng},
	urldate = {2024-11-17},
	date = {2024-01-28},
	eprinttype = {arxiv},
	eprint = {2401.13275},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/S72HZWNJ/Can AI Assistants Know What They Don't Know - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/Q4K9NNXH/2401.html:text/html},
}


@misc{KnowledgeVerificationNipHallucinationBud2024,
	title = {Knowledge Verification to Nip Hallucination in the Bud},
	url = {http://arxiv.org/abs/2401.10768},
	abstract = {While large language models ({LLMs}) have demonstrated exceptional performance across various tasks following human alignment, they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination. In this paper, we demonstrate the feasibility of mitigating hallucinations by verifying and minimizing the inconsistency between external knowledge present in the alignment data and the intrinsic knowledge embedded within foundation {LLMs}. Specifically, we propose a novel approach called Knowledge Consistent Alignment ({KCA}), which employs a well-aligned {LLM} to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation {LLMs}. To address knowledge inconsistencies in the alignment data, {KCA} implements several specific strategies to deal with these data instances. We demonstrate the superior efficacy of {KCA} in reducing hallucinations across six benchmarks, utilizing foundation {LLMs} of varying backbones and scales. This confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency. Our code, model weights, and data are openly accessible at {\textbackslash}url\{https://github.com/fanqiwan/{KCA}\}.},
	number = {{arXiv}:2401.10768},
	publisher = {{arXiv}},
	author = {Wan, Fanqi and Huang, Xinting and Cui, Leyang and Quan, Xiaojun and Bi, Wei and Shi, Shuming},
	urldate = {2024-11-17},
	date = {2024-09-22},
	eprinttype = {arxiv},
	eprint = {2401.10768},
	keywords = {/unread, Computer Science - Computation and Language},
}


@misc{UnfamiliarFinetuningExamplesControlHow2024,
	title = {Unfamiliar Finetuning Examples Control How Language Models Hallucinate},
	url = {http://arxiv.org/abs/2403.05612},
	doi = {10.48550/arXiv.2403.05612},
	abstract = {Large language models are known to hallucinate when faced with unfamiliar queries, but the underlying mechanism that govern how models hallucinate are not yet fully understood. In this work, we find that unfamiliar examples in the models' finetuning data -- those that introduce concepts beyond the base model's scope of knowledge -- are crucial in shaping these errors. In particular, we find that an {LLM}'s hallucinated predictions tend to mirror the responses associated with its unfamiliar finetuning examples. This suggests that by modifying how unfamiliar finetuning examples are supervised, we can influence a model's responses to unfamiliar queries (e.g., say ``I don't know''). We empirically validate this observation in a series of controlled experiments involving {SFT}, {RL}, and reward model finetuning on {TriviaQA} and {MMLU}. Our work further investigates {RL} finetuning strategies for improving the factuality of long-form model generations. We find that, while hallucinations from the reward model can significantly undermine the effectiveness of {RL} factuality finetuning, strategically controlling how reward models hallucinate can minimize these negative effects. Leveraging our previous observations on controlling hallucinations, we propose an approach for learning more reliable reward models, and show that they improve the efficacy of {RL} factuality finetuning in long-form biography and book/movie plot generation tasks.},
	number = {{arXiv}:2403.05612},
	publisher = {{arXiv}},
	author = {Kang, Katie and Wallace, Eric and Tomlin, Claire and Kumar, Aviral and Levine, Sergey},
	urldate = {2024-11-17},
	date = {2024-05-28},
	eprinttype = {arxiv},
	eprint = {2403.05612},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/IRSJ3CNV/Unfamiliar Finetuning Examples Control How Language Models Hallucinate - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/NDZI83SW/2403.html:text/html},
}


@misc{tianFinetuningLanguageModels2023,
	title = {Fine-tuning Language Models for Factuality},
	url = {http://arxiv.org/abs/2311.08401},
	doi = {10.48550/arXiv.2311.08401},
	abstract = {The fluency and creativity of large pre-trained language models ({LLMs}) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in {NLP} to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with {RLHF} or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58\% and 40\% reduction in factual error rate when generating biographies and answering medical questions, respectively.},
	number = {{arXiv}:2311.08401},
	publisher = {{arXiv}},
	author = {Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D. and Finn, Chelsea},
	urldate = {2024-11-14},
	date = {2023-11-14},
	eprinttype = {arxiv},
	eprint = {2311.08401},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/HXHQURGA/Fine-tuning Language Models for Factuality - 2023.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/EUMQHJEQ/2311.html:text/html},
}


@misc{tianFinetuningLanguageModels2023,
	title = {Fine-tuning Language Models for Factuality},
	url = {http://arxiv.org/abs/2311.08401},
	doi = {10.48550/arXiv.2311.08401},
	abstract = {The fluency and creativity of large pre-trained language models ({LLMs}) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in {NLP} to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with {RLHF} or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58\% and 40\% reduction in factual error rate when generating biographies and answering medical questions, respectively.},
	number = {{arXiv}:2311.08401},
	publisher = {{arXiv}},
	author = {Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D. and Finn, Chelsea},
	urldate = {2024-11-14},
	date = {2023-11-14},
	eprinttype = {arxiv},
	eprint = {2311.08401},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/HXHQURGA/Fine-tuning Language Models for Factuality - 2023.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/EUMQHJEQ/2311.html:text/html},
}



@inproceedings{SelfAlignmentFactualityMitigatingHallucinationsLLMs2024,
	location = {Bangkok, Thailand},
	title = {Self-Alignment for Factuality: Mitigating Hallucinations in {LLMs} via Self-Evaluation},
	url = {https://aclanthology.org/2024.acl-long.107},
	doi = {10.18653/v1/2024.acl-long.107},
	shorttitle = {Self-Alignment for Factuality},
	abstract = {Despite showing impressive abilities, large language models ({LLMs}) often struggle with factual inaccuracies, i.e., ”hallucinations”, even when they hold relevant knowledge. To mitigate these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an {LLM} to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an {LLM} to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning ({SK}-Tuning) to augment the {LLM}'s self-evaluation ability by improving the model's confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm. We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on {TruthfulQA} and {BioGEN}.},
	eventtitle = {{ACL} 2024},
	pages = {1946--1965},
	booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Xiaoying and Peng, Baolin and Tian, Ye and Zhou, Jingyan and Jin, Lifeng and Song, Linfeng and Mi, Haitao and Meng, Helen},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	urldate = {2024-11-18},
	date = {2024-08},
	keywords = {/unread},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/GX26QTCV/Self-Alignment for Factuality Mitigating Hallucinations in LLMs via Self-Evaluation - 2024.pdf:application/pdf},
}



@misc{LargeLanguageModelsCannotSelfCorrect2024,
	title = {Large Language Models Cannot Self-Correct Reasoning Yet},
	url = {http://arxiv.org/abs/2310.01798},
	abstract = {Large Language Models ({LLMs}) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within {LLMs}, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an {LLM} attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that {LLMs} struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.},
	number = {{arXiv}:2310.01798},
	publisher = {{arXiv}},
	author = {Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
	urldate = {2024-11-18},
	date = {2024-03-14},
	eprinttype = {arxiv},
	eprint = {2310.01798},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/MSGFJY4D/Large Language Models Cannot Self-Correct Reasoning Yet - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/ZWA49DZ3/2310.html:text/html},
}


@misc{SELFINCORRECTLLMsStruggleDiscriminatingSelfGenerated2024,
	title = {{SELF}-[{IN}]{CORRECT}: {LLMs} Struggle with Discriminating Self-Generated Responses},
	url = {http://arxiv.org/abs/2404.04298},
	doi = {10.48550/arXiv.2404.04298},
	shorttitle = {{SELF}-[{IN}]{CORRECT}},
	abstract = {Can {LLMs} consistently improve their previous outputs for better results? For this to be true, {LLMs} would need to be better at discriminating among previously-generated alternatives, than generating initial responses. We explore the validity of this hypothesis in practice. We first formulate a unified framework that allows us to compare the generative and discriminative capability of any model on any task. In our resulting experimental analysis of several open-source and industrial {LLMs}, we observe that models are not reliably better at discriminating among previously-generated alternatives than generating initial responses. This finding challenges the notion that {LLMs} may be able to enhance their performance only through their own judgment.},
	number = {{arXiv}:2404.04298},
	publisher = {{arXiv}},
	author = {Jiang, Dongwei and Zhang, Jingyu and Weller, Orion and Weir, Nathaniel and Durme, Benjamin Van and Khashabi, Daniel},
	urldate = {2024-11-18},
	date = {2024-09-06},
	eprinttype = {arxiv},
	eprint = {2404.04298},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/ADBMQI68/SELF-[IN]CORRECT LLMs Struggle with Discriminating Self-Generated Responses - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/EL3XRG7N/2404.html:text/html},
}

@article{weng2024hallucination,
  title   = "Extrinsic Hallucinations in LLMs.",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2024",
  month   = "Jul",
  url     = "https://lilianweng.github.io/posts/2024-07-07-hallucination/"
}


@misc{SurveyHonestyLargeLanguageModels2024,
	title = {A Survey on the Honesty of Large Language Models},
	url = {http://arxiv.org/abs/2409.18786},
	abstract = {Honesty is a fundamental principle for aligning large language models ({LLMs}) with human values, requiring these models to recognize what they know and don't know and be able to faithfully express their knowledge. Despite promising, current {LLMs} still exhibit significant dishonest behaviors, such as confidently presenting wrong answers or failing to express what they know. In addition, research on the honesty of {LLMs} also faces challenges, including varying definitions of honesty, difficulties in distinguishing between known and unknown knowledge, and a lack of comprehensive understanding of related research. To address these issues, we provide a survey on the honesty of {LLMs}, covering its clarification, evaluation approaches, and strategies for improvement. Moreover, we offer insights for future research, aiming to inspire further exploration in this important area.},
	number = {{arXiv}:2409.18786},
	publisher = {{arXiv}},
	author = {Li, Siheng and Yang, Cheng and Wu, Taiqiang and Shi, Chufan and Zhang, Yuji and Zhu, Xinyu and Cheng, Zesen and Cai, Deng and Yu, Mo and Liu, Lemao and Zhou, Jie and Yang, Yujiu and Wong, Ngai and Wu, Xixin and Lam, Wai},
	urldate = {2024-11-17},
	date = {2024-09-27},
	eprinttype = {arxiv},
	eprint = {2409.18786},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/CWIN2SD8/A Survey on the Honesty of Large Language Models - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/V6SPEFTW/2409.html:text/html},
}


@article{UNDERSTANDINGFACTUALKNOWLEDGELARGELANGUAGE2024a,
	title = {{TOWARDS} {UNDERSTANDING} {FACTUAL} {KNOWLEDGE} {OF} {LARGE} {LANGUAGE} {MODELS}},
	abstract = {Large language models ({LLMs}) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases ({KBs}) that explicitly store factual knowledge, {LLMs} implicitly store facts in their parameters. Content generated by the {LLMs} can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to explore the extent and scope of factual knowledge within {LLMs} by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether {LLMs} can compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identify subtle factual differences, and resist adversarial examples. Extensive experiments on different sizes and types of {LLMs} show that existing {LLMs} still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes are publicly available at: https://github.com/{THU}-{BPM}/Pinocchio.},
	author = {Hu, Xuming and Chen, Junzhe and Li, Xiaochuan and Guo, Yufei and Wen, Lijie and Yu, Philip S and Guo, Zhijiang},
	date = {2024},
	langid = {english},
	file = {PDF:/Users/eltonli/Zotero/storage/GDQRDQMY/TOWARDS UNDERSTANDING FACTUAL KNOWLEDGE OF LARGE LANGUAGE MODELS - 2024.pdf:application/pdf},
}


@misc{OnePreferenceFitsAllAlignmentMultiObjectiveDirectPreference2024,
	title = {Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization},
	url = {http://arxiv.org/abs/2310.03708},
	doi = {10.48550/arXiv.2310.03708},
	shorttitle = {Beyond One-Preference-Fits-All Alignment},
	abstract = {A single language model, even when aligned with labelers through reinforcement learning from human feedback ({RLHF}), may not suit all human preferences. Recent approaches therefore prefer customization, gathering multi-dimensional feedback, and creating distinct reward models for each dimension. Different language models are then optimized for various preferences using multi-objective {RLHF} ({MORLHF}) with varying reward weights. However, {RL} fine-tuning is unstable and resource-heavy, especially with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization ({MODPO}), an {RL}-free extension of Direct Preference Optimization ({DPO}) for multiple alignment objectives. Essentially, {MODPO} folds language modeling directly into reward modeling, training language models as implicit collective reward models that combine all objectives with specific weights. {MODPO} theoretically yields the same optimal solutions as {MORLHF} but is practically more stable and efficient. Empirical results in safety alignment and long-form question answering show that {MODPO} matches or outperforms existing methods, producing a Pareto front of language models catering to diverse preferences with three times less computational resources compared to {MORLHF}. Code is available at https://github.com/{ZHZisZZ}/modpo.},
	number = {{arXiv}:2310.03708},
	publisher = {{arXiv}},
	author = {Zhou, Zhanhui and Liu, Jie and Shao, Jing and Yue, Xiangyu and Yang, Chao and Ouyang, Wanli and Qiao, Yu},
	urldate = {2024-11-25},
	date = {2024-08-17},
	eprinttype = {arxiv},
	eprint = {2310.03708},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/9YZUG5GA/Beyond One-Preference-Fits-All Alignment Multi-Objective Direct Preference Optimization - 2024.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/QLMWIMMQ/2310.html:text/html},
}


@misc{FActScoreFinegrainedAtomicEvaluationFactual2023,
	title = {{FActScore}: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation},
	url = {http://arxiv.org/abs/2305.14251},
	doi = {10.48550/arXiv.2305.14251},
	shorttitle = {{FActScore}},
	abstract = {Evaluating the factuality of long-form text generated by large language models ({LMs}) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce {FACTSCORE}, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain {FACTSCOREs} of people biographies generated by several state-of-the-art commercial {LMs} -- {InstructGPT}, {ChatGPT}, and the retrieval-augmented {PerplexityAI} -- and report new analysis demonstrating the need for such a fine-grained score (e.g., {ChatGPT} only achieves 58\%). Since human evaluation is costly, we also introduce an automated model that estimates {FACTSCORE} using retrieval and a strong language model, with less than a 2\% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent {LMs} that would have cost \$26K if evaluated by humans, with various findings: {GPT}-4 and {ChatGPT} are more factual than public models, and Vicuna and Alpaca are some of the best public models. {FACTSCORE} is available for public use via `pip install factscore`.},
	number = {{arXiv}:2305.14251},
	publisher = {{arXiv}},
	author = {Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
	urldate = {2024-11-25},
	date = {2023-10-11},
	eprinttype = {arxiv},
	eprint = {2305.14251},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}


@misc{FinegrainedHallucinationDetectionEditingLanguage2024,
	title = {Fine-grained Hallucination Detection and Editing for Language Models},
	url = {http://arxiv.org/abs/2401.06855},
	doi = {10.48550/arXiv.2401.06855},
	abstract = {Large language models ({LMs}) are prone to generate factual errors, which are often called hallucinations. In this paper, we introduce a comprehensive taxonomy of hallucinations and argue that hallucinations manifest in diverse forms, each requiring varying degrees of careful assessments to verify factuality. We propose a novel task of automatic fine-grained hallucination detection and construct a new evaluation benchmark, {FavaBench}, that includes about one thousand fine-grained human judgments on three {LM} outputs across various domains. Our analysis reveals that {ChatGPT} and Llama2-Chat (70B, 7B) exhibit diverse types of hallucinations in the majority of their outputs in information-seeking scenarios. We train {FAVA}, a retrieval-augmented {LM} by carefully creating synthetic data to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that {FAVA} significantly outperforms {ChatGPT} and {GPT}-4 on fine-grained hallucination detection, and edits suggested by {FAVA} improve the factuality of {LM}-generated text.},
	number = {{arXiv}:2401.06855},
	publisher = {{arXiv}},
	author = {Mishra, Abhika and Asai, Akari and Balachandran, Vidhisha and Wang, Yizhong and Neubig, Graham and Tsvetkov, Yulia and Hajishirzi, Hannaneh},
	urldate = {2024-11-25},
	date = {2024-08-12},
	eprinttype = {arxiv},
	eprint = {2401.06855},
	keywords = {/unread, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/2QGXI84Z/Fine-grained Hallucination Detection and Editing for Language Models - 2024.pdf:application/pdf},
}


@misc{OpenAssistantConversationsDemocratizingLargeLanguage2023,
	title = {{OpenAssistant} Conversations -- Democratizing Large Language Model Alignment},
	url = {http://arxiv.org/abs/2304.07327},
	doi = {10.48550/arXiv.2304.07327},
	abstract = {Aligning large language models ({LLMs}) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by {ChatGPT}. Alignment techniques such as supervised fine-tuning ({SFT}) and reinforcement learning from human feedback ({RLHF}) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of {LLMs}, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like {RLHF} rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release {OpenAssistant} Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. Models trained on {OpenAssistant} Conversations show consistent improvements on standard benchmarks over respective base models. We release our code and data under a fully permissive licence.},
	number = {{arXiv}:2304.07327},
	publisher = {{arXiv}},
	author = {Köpf, Andreas and Kilcher, Yannic and Rütte, Dimitri von and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Richárd and {ES}, Shahul and Suri, Sameer and Glushkov, David and Dantuluri, Arnav and Maguire, Andrew and Schuhmann, Christoph and Nguyen, Huu and Mattick, Alexander},
	urldate = {2024-11-25},
	date = {2023-10-31},
	eprinttype = {arxiv},
	eprint = {2304.07327},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/XAIKX7M2/OpenAssistant Conversations -- Democratizing Large Language Model Alignment - 2023.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/5CIG964K/2304.html:text/html},
}

@InProceedings{pmlr-v174-pal22a,
  title = 	 {MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering},
  author =       {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  booktitle = 	 {Proceedings of the Conference on Health, Inference, and Learning},
  pages = 	 {248--260},
  year = 	 {2022},
  editor = 	 {Flores, Gerardo and Chen, George H and Pollard, Tom and Ho, Joyce C and Naumann, Tristan},
  volume = 	 {174},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07--08 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v174/pal22a/pal22a.pdf},
  url = 	 {https://proceedings.mlr.press/v174/pal22a.html},
  abstract = 	 {This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects & topics. A detailed explanation of the solution, along with the above information, is provided in this study.}
}

@misc{TruthfulQAMeasuringHowModelsMimic2022,
	title = {{TruthfulQA}: Measuring How Models Mimic Human Falsehoods},
	url = {http://arxiv.org/abs/2109.07958},
	doi = {10.48550/arXiv.2109.07958},
	shorttitle = {{TruthfulQA}},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested {GPT}-3, {GPT}-Neo/J, {GPT}-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other {NLP} tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	number = {{arXiv}:2109.07958},
	publisher = {{arXiv}},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	urldate = {2024-11-25},
	date = {2022-05-08},
	eprinttype = {arxiv},
	eprint = {2109.07958},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/eltonli/Zotero/storage/U8IC6D9L/TruthfulQA Measuring How Models Mimic Human Falsehoods - 2022.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/93J67HTR/2109.html:text/html},
}


@misc{minFActScoreFinegrainedAtomic2023,
	title = {{FActScore}: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation},
	url = {http://arxiv.org/abs/2305.14251},
	shorttitle = {{FActScore}},
	abstract = {Evaluating the factuality of long-form text generated by large language models ({LMs}) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce {FACTSCORE}, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain {FACTSCOREs} of people biographies generated by several state-of-the-art commercial {LMs} -- {InstructGPT}, {ChatGPT}, and the retrieval-augmented {PerplexityAI} -- and report new analysis demonstrating the need for such a fine-grained score (e.g., {ChatGPT} only achieves 58\%). Since human evaluation is costly, we also introduce an automated model that estimates {FACTSCORE} using retrieval and a strong language model, with less than a 2\% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent {LMs} that would have cost \$26K if evaluated by humans, with various findings: {GPT}-4 and {ChatGPT} are more factual than public models, and Vicuna and Alpaca are some of the best public models. {FACTSCORE} is available for public use via `pip install factscore`.},
	number = {{arXiv}:2305.14251},
	publisher = {{arXiv}},
	author = {Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
	urldate = {2024-11-16},
	date = {2023-10-11},
	eprinttype = {arxiv},
	eprint = {2305.14251},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, /unread},
	file = {Full Text PDF:/Users/eltonli/Zotero/storage/2V4R2WI3/FActScore Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation - 2023.pdf:application/pdf;Snapshot:/Users/eltonli/Zotero/storage/NQTVV2NE/2305.html:text/html},
}
